# Language Models are Unsupervised Multitask Learners
## Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever
https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
___
### Abstract
> ONatural language processing tasks, such as ques-tion answering, machine translation, reading com-prehension,  and  summarization,  are  typicallyapproached  with  supervised  learning  on  task-specific datasets. We demonstrate that languagemodels begin to learn these tasks without any ex-plicit supervision when trained on a new datasetof millions of webpages called WebText. Whenconditioned on a document plus questions, the an-swers generated by the language model reach 55F1 on the CoQA dataset - matching or exceedingthe performance of 3 out of 4 baseline systemswithout  using  the  127,000+  training  examples.The capacity of the language model is essentialto the success of zero-shot task transfer and in-creasing it improves performance in a log-linearfashion across tasks. Our largest model, GPT-2,is a 1.5B parameter Transformer that achievesstate of the art results on 7 out of 8 tested lan-guage modeling datasets in a zero-shot settingbut still underfits WebText.   Samples from themodel reflect these improvements and contain co-herent paragraphs of text. These findings suggesta promising path towards building language pro-cessing systems which learn to perform tasks fromtheir naturally occurring demonstrations.
___
